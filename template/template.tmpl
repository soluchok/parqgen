package {{.PackageName}}

import (
	"fmt"
	"io"

	"github.com/apache/arrow/go/v11/parquet"
	"github.com/apache/arrow/go/v11/parquet/file"
	"github.com/apache/arrow/go/v11/parquet/schema"
)

{{- $structName := .TypeName}}

type values struct {
	values interface{}
	defLev []int16
	repLev []int16
}

type {{$structName}}Writer struct {
	schema *schema.Schema
	keys   [][][]interface{}
}

func New{{$structName}}Writer() *{{$structName}}Writer {
	_schema, err := schema.NewSchemaFromStruct(QueryData{})
	if err != nil {
		panic(err)
	}

	var nodes schema.FieldList

	for i := 0; i < _schema.Root().NumFields(); i++ {
		nodes = append(nodes, _schema.Root().Field(i))
	}

	root, err := schema.NewGroupNode(_schema.Root().Name(), parquet.Repetitions.Required, nodes, -1)
	if err != nil {
		panic(err)
	}

	return &{{$structName}}Writer{schema: schema.NewSchema(root)}
}

{{range $value := .Values -}}
func (s *{{$structName}}Writer) _{{$value.Name}}{{$value.Path}}(data []{{$structName}}) values {
	var n = len(data)

    {{ $stackLen := len .Stack }} {{ if ne $stackLen 0 }}
        n = 0
        for i := range data {
            var val0 = data[i].{{$value.Name}}
            {{range $idx, $stackValue := .Stack -}}
                if len(val{{$idx}}) == 0 {
                    n++
                }

                {{ if eq $stackValue.Suffix "_mapKey" }}
                for val{{inc $idx}} := range val{{$idx}} {
                {{ else }}
                for _, val{{inc $idx}} := range val{{$idx}} {
                {{ end }}


                    {{ if eq $stackLen (inc $idx) }}
                        _ = val{{inc $idx}}
                        n++
                    {{ end }}
            {{end -}}
            {{range $idx, $stackValue := .Stack -}}
                }
            {{end -}}
        }
    {{ end }}

	var (
		_values = make([]{{$value.Type}}, n)
		_def    = make([]int16, n)
		_rep    = make([]int16, n)
	)

    n = 0
	var nv int
_ = nv

	for i := range data {
	        {{ if eq $value.Suffix "_mapKey" }}
	        		if i >= len(s.keys) {
            			s.keys = append(s.keys, [][]interface{}{})
            		}
            {{ end }}

	        {{ $stackLen := len .Stack }} {{ if ne $stackLen 0 }}
	                var _lastRep int16
                    var val0 = data[i].{{$value.Name}}

                    {{range $idx, $stackValue := .Stack -}}
                        {{ if eq $stackValue.Suffix "_mapValue" }}


                            		var loc{{$idx}} []interface{}
                            		if len(s.keys[i]) > {{$idx}} {
                            			loc{{$idx}} = s.keys[i][{{$idx}}]
                            		}

                        {{ end }}
                    {{ end }}

                    {{range $idx, $stackValue := .Stack -}}
                        if len(val{{$idx}}) == 0 {
                            _def[n] = {{$idx}}
                            _rep[n] = _lastRep
                            _lastRep = {{inc $idx}}
                            n++
                        }

                        {{ if eq $stackValue.Suffix "_mapKey" }}
                            var c{{$idx}} int
                            for val{{inc $idx}} := range val{{$idx}} {
                            if {{$idx}} >= len(s.keys[i]) {
                                s.keys[i] = append(s.keys[i], make([][]interface{}, {{inc $idx}}-len(s.keys[i]))...)
                            }

                            s.keys[i][{{$idx}}] = append(s.keys[i][{{$idx}}], val{{inc $idx}})


                            c{{$idx}}++

                        {{ else if eq $stackValue.Suffix "_mapValue"}}

                                for _, val{{inc $idx}} := range val{{$idx}} {

                                        val{{inc $idx}} = val{{$idx}}[loc{{$idx}}[0].(string)]
                                        loc{{$idx}} = loc{{$idx}}[1:]

                                        {{ if eq $value.Suffix "_mapValue" }}
                                			val{{inc $idx}} = val{{$idx}}[s.keys[i][{{$idx}}][0].(string)]
                                			s.keys[i][{{$idx}}] = s.keys[i][{{$idx}}][1:]
                                        {{ end }}

                        {{ else }}
                            for _, val{{inc $idx}} := range val{{$idx}} {
                        {{ end }}


                            {{ if eq $stackLen (inc $idx) }}
                               	_values[nv] = {{$value.Type}}(val{{inc $idx}})
                                _def[n] = {{inc $idx}}
                                _rep[n] = _lastRep
                                _lastRep = {{inc $idx}}

                                nv++
                                n++
                            {{ end }}
                    {{end -}}
                    {{range $idx, $stackValue := .Stack -}}
                        }
                        _lastRep--
                    {{end -}}
             {{ else }}
             _values[i] = {{$value.Type}}(data[i].{{$value.Name}})
            {{ end }}
	}

	return values{values: _values, defLev: _def, repLev: _rep}
}

{{end -}}

func (s *{{$structName}}Writer) values(data []{{$structName}}) []values {
	return []values{
	  {{- range $value := .Values}}
		s._{{$value.Name}}{{$value.Path}}(data),
	  {{- end}}
	}
}

func (s *{{$structName}}Writer) Write(out io.Writer, values []QueryData) {
	var writer = file.NewParquetWriter(out, s.schema.Root(), file.WithWriterProps(
		parquet.NewWriterProperties(
		    parquet.WithVersion(parquet.V1_0),
		    parquet.WithRootRepetition(parquet.Repetition(0)),
		),
	))
	defer writer.Close()

	var rgw = writer.AppendRowGroup()
	defer rgw.Close()

	for _, val := range s.values(values) {
		cw, err := rgw.NextColumn()
		if err != nil {
			panic(err)
		}

		switch w := cw.(type) {
		case *file.Int64ColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]int64), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.Int32ColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]int32), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.ByteArrayColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]parquet.ByteArray), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.Float64ColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]float64), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.BooleanColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]bool), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		default:
			panic(fmt.Sprintf("unimplemented: %T", w))
		}

		cw.Close()
	}
}
