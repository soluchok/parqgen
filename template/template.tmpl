package {{.PackageName}}

import (
	"fmt"
	"io"
	"reflect"
	"unsafe"

	"github.com/apache/arrow/go/v11/parquet"
	"github.com/apache/arrow/go/v11/parquet/file"
	"github.com/apache/arrow/go/v11/parquet/schema"
)

{{- $structName := .TypeName}}

type values struct {
	values interface{}
	defLev []int16
	repLev []int16
}

type {{$structName}}Writer struct {
	schema      *schema.Schema
	properties []WriterProperty
	keys        [][][]interface{}
}

type WriterProperty = parquet.WriterProperty

func WithDictionaryFor(path string, dict bool) WriterProperty {
	return parquet.WithDictionaryFor(path, dict)
}

func WithDataPageSize(pgsize int64) WriterProperty {
	return parquet.WithDataPageSize(pgsize)
}

func New{{$structName}}Writer(opts ...WriterProperty) *{{$structName}}Writer {
	_schema, err := schema.NewSchemaFromStruct({{$structName}}{})
	if err != nil {
		panic(err)
	}

	var nodes schema.FieldList

	for i := 0; i < _schema.Root().NumFields(); i++ {
		nodes = append(nodes, _schema.Root().Field(i))
	}

	root, err := schema.NewGroupNode(_schema.Root().Name(), parquet.Repetitions.Required, nodes, -1)
	if err != nil {
		panic(err)
	}

    var properties = []WriterProperty{
        parquet.WithVersion(parquet.V1_0),
        parquet.WithRootRepetition(parquet.Repetition(0)),
        parquet.WithDictionaryDefault(false),
    }

	return &{{$structName}}Writer{schema: schema.NewSchema(root), properties: append(properties, opts...)}
}

func unsafeByteArray(s string) parquet.ByteArray {
	if len(s) == 0 {
		return nil
	}

	return (*[0x7fff0000]byte)(unsafe.Pointer(
		(*reflect.StringHeader)(unsafe.Pointer(&s)).Data),
	)[:len(s):len(s)]
}

{{range $value := .Values}}
	func (s *{{$structName}}Writer) _{{$value.Path}}(data []{{$structName}}) values {
		{{ $hasStack := ne (len .Stack) 0 }}

		{{- if not $hasStack -}}
			var _values = make([]{{$value.Type}}, len(data))

			for i := range data {
				{{- if eq $value.Type "parquet.ByteArray" -}}
					_values[i] = unsafeByteArray(data[i].{{$value.FieldName}})
				{{- else -}}
					_values[i] =  data[i].{{$value.FieldName}}
				{{- end -}}
			}

			return values{values: _values}
		{{ else -}}
			var num int
			for i := range data {
				var val0 = data[i].{{$value.FieldName}}
				{{range $idx, $stackValue := .Stack -}}
					if len(val{{$idx}}) == 0 {
						num++
						continue
					}

					{{ if eq (len $value.Stack) (inc $idx) }}
						num += len(val{{$idx}})
					{{- else }}
						for _, val{{inc $idx}} := range val{{$idx}} {
					{{ end -}}
				{{ end }}

				{{- range $idx, $stackValue := .Stack -}}
					{{- if ne (len $value.Stack) (inc $idx) }}
						}
					{{- end -}}
				{{ end }}
			}

			var (
				_values = make([]{{$value.Type}}, num)
				_def    = make([]int16, num)
				_rep    = make([]int16, num)
			)

			var n, nv int

			for i := range data {
				var _lastRep int16
				var val0 = data[i].{{$value.FieldName}}

				{{range $idx, $stackValue := .Stack -}}
					{{ if eq $stackValue.Type "MapValue" }}
						var loc{{$idx}} []interface{}
						if len(s.keys[i]) > {{$idx}} {
							loc{{$idx}} = s.keys[i][{{$idx}}]
						}
					{{ end }}
				{{ end }}

				{{range $idx, $stackValue := .Stack -}}
					if len(val{{$idx}}) == 0 {
						_def[n], _rep[n], _lastRep = {{$idx}}, _lastRep, {{inc $idx}}
						n++
					}

					{{ if eq $stackValue.Type "MapKey" }}
						for val{{inc $idx}} := range val{{$idx}} {
							if {{$idx}} >= len(s.keys[i]) {
								s.keys[i] = append(s.keys[i], make([][]interface{}, {{inc $idx}}-len(s.keys[i]))...)
							}

							s.keys[i][{{$idx}}] = append(s.keys[i][{{$idx}}], val{{inc $idx}})
					{{ else if eq $stackValue.Type "MapValue"}}
						for _, val{{inc $idx}} := range val{{$idx}} {
							val{{inc $idx}}, loc{{$idx}} = val{{$idx}}[loc{{$idx}}[0].(string)], loc{{$idx}}[1:]
					{{ else }}
						for _, val{{inc $idx}} := range val{{$idx}} {
					{{ end }}

					{{- if eq (len $value.Stack) (inc $idx) -}}

						{{ if eq $value.Type "parquet.ByteArray" }}
							_values[nv] = unsafeByteArray(val{{inc $idx}})
						{{ else }}
							_values[nv] =  val{{inc $idx}}
						{{ end }}

						_def[n], _rep[n], _lastRep = {{inc $idx}}, _lastRep, {{inc $idx}}

						nv++
						n++
					{{- end -}}
				{{end -}}

				{{- range $idx, $stackValue := .Stack -}}
					}
					_lastRep--
				{{- end -}}

				{{ if $value.ReleaseKeys }}
					s.keys[i] = s.keys[i][len(s.keys[i]):]
				{{- end }}
			}

			return values{values: _values, defLev: _def, repLev: _rep}
		{{ end -}}
	}
{{end}}

func (s *{{$structName}}Writer) values(data []{{$structName}}) []values {
	return []values{
	  {{- range $value := .Values}}
		s._{{$value.Path}}(data),
	  {{- end}}
	}
}

func (s *{{$structName}}Writer) Write(out io.Writer, values []{{$structName}}) {
	var writer = file.NewParquetWriter(out, s.schema.Root(), file.WithWriterProps(
		parquet.NewWriterProperties(s.properties...),
	))
	defer writer.Close()

	var rgw = writer.AppendRowGroup()
	defer rgw.Close()

    s.keys = make([][][]interface{}, len(values))

	for _, val := range s.values(values) {
		cw, err := rgw.NextColumn()
		if err != nil {
			panic(err)
		}

		switch w := cw.(type) {
		case *file.Int64ColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]int64), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.Int32ColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]int32), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.ByteArrayColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]parquet.ByteArray), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.Float64ColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]float64), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		case *file.BooleanColumnChunkWriter:
			_, err = w.WriteBatch(val.values.([]bool), val.defLev, val.repLev)
			if err != nil {
				panic(err)
			}
		default:
			panic(fmt.Sprintf("unimplemented: %T", w))
		}

		cw.Close()
	}
}
